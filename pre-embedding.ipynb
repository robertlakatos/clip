{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce08c42-de04-4e25-9cc6-29945f882329",
   "metadata": {},
   "source": [
    "# Data Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f251c9c-8209-4abe-a031-d26c63fd1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e23ca-575d-46ba-9fc7-f08d877897aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694ced5-2090-4f9f-9e62-61eed4d214cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image and text encoders\n",
    "image_encoder = models.resnet50(pretrained=True)\n",
    "image_encoder.fc = torch.nn.Identity()\n",
    "image_encoder = image_encoder.to(\"cuda\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317485da-80cd-499d-b12c-19ed90a2abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = AutoModel.from_pretrained(\"bert-base-uncased\").to(\"cuda\").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f9d173-63cb-40d0-8085-639de31d5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_embedder(image_folder, output_file):\n",
    "    image_embeddings = {}\n",
    "    \n",
    "    for image_name in tqdm(os.listdir(image_folder)):\n",
    "        image_path = os.path.join(image_folder, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = transform(image).unsqueeze(0).to(\"cuda\")\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            image_embedding = image_encoder(image_tensor).squeeze(0).cpu()  # Move back to CPU for saving\n",
    "            image_embeddings[image_name] = image_embedding\n",
    "\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(image_embeddings, f)\n",
    "\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311de062-d705-4425-a46f-18460acea5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_image_embedder(dataset, output_file):\n",
    "    image_embeddings = {}\n",
    "    \n",
    "    for idx, (image_tensor, label) in enumerate(tqdm(dataset)):    \n",
    "        with torch.no_grad():\n",
    "            image_embedding = image_encoder(image_tensor.unsqueeze(0).to(\"cuda\")).squeeze(0).cpu()  # Move back to CPU for saving\n",
    "            image_embeddings[idx] = {\n",
    "                \"label\" : label,\n",
    "                \"emb\" : image_embedding\n",
    "            }\n",
    "\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(image_embeddings, f)\n",
    "\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e718045-b969-4e0f-b0f4-09322c87a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_text_embedder(caption_file, output_file):\n",
    "    with open(caption_file, 'r') as file:\n",
    "        captions = json.load(file)\n",
    "\n",
    "    annotations = [captions[\"annotations\"][i][\"caption\"] for i in range(len(captions[\"annotations\"]))]\n",
    "    annotations = pd.DataFrame(annotations)\n",
    "    annotations[\"len\"] = annotations[0].apply(lambda x : len(tokenizer(x)[\"input_ids\"]))\n",
    "\n",
    "    max_length = int(annotations.describe()[\"len\"].values[-1]) + 1\n",
    "    print(\"max_length\", max_length)\n",
    "    \n",
    "    img_filenames = {captions[\"images\"][i][\"id\"] : captions[\"images\"][i][\"file_name\"] for i in range(len(captions[\"images\"]))}\n",
    "\n",
    "    text_embeddings = {}\n",
    "    for i in tqdm(range(len(captions[\"images\"]))):\n",
    "        tokens = tokenizer(captions[\"annotations\"][i][\"caption\"], \n",
    "                           return_tensors=\"pt\",\n",
    "                           padding=True, \n",
    "                           truncation=True, \n",
    "                           max_length=max_length).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            text_embedding = text_encoder(**tokens).last_hidden_state[:, 0, :].squeeze(0).cpu()  # Move back to CPU\n",
    "            text_embeddings[captions[\"annotations\"][i][\"id\"]] = {\n",
    "                \"img_fn\" : img_filenames[captions[\"annotations\"][i][\"image_id\"]],\n",
    "                \"emb\" : text_embedding,\n",
    "                \"caption\" : captions[\"annotations\"][i][\"caption\"], \n",
    "            }\n",
    "\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(text_embeddings, f)\n",
    "\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc456b-c16d-4dc5-86f9-a8e521d012d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flickr30k_text_embedder(caption_file, output_file):\n",
    "    flickr30k_captions = pd.read_csv(caption_file)\n",
    "    flickr30k_captions[\"len\"] = flickr30k_captions[\"comment\"].apply(lambda x : len(tokenizer(x)[\"input_ids\"]))\n",
    "    max_length = int(flickr30k_captions.describe()[\"len\"].values[-1]) + 1\n",
    "    print(\"max_length\", max_length)\n",
    "    \n",
    "    text_embeddings = {}\n",
    "    for i in tqdm(range(len(flickr30k_captions))):\n",
    "        tokens = tokenizer(flickr30k_captions[\"comment\"].values[i], \n",
    "                           return_tensors=\"pt\",\n",
    "                           padding=True, \n",
    "                           truncation=True, \n",
    "                           max_length=max_length).to(\"cuda\")\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            text_embedding = text_encoder(**tokens).last_hidden_state[:, 0, :].squeeze(0).cpu()  # Move back to CPU\n",
    "            text_embeddings[i] = {\n",
    "                \"comment_number\" : flickr30k_captions[\"comment_number\"].values[i],\n",
    "                \"emb\" : text_embedding,\n",
    "                \"comment\" : flickr30k_captions[\"comment\"].values[i],\n",
    "                \"image_name\" : flickr30k_captions[\"image_name\"].values[i]\n",
    "            }\n",
    "    \n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(text_embeddings, f)\n",
    "\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c3b57-cd72-430f-b89d-76f0be3d3148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_text_embedder(dataset, output_file):\n",
    "    max_length = max([len(tokenizer(name)[\"input_ids\"]) for name in dataset.classes])\n",
    "    print(\"max_length\", max_length)\n",
    "    \n",
    "    text_embeddings = {}\n",
    "    for idx, name in enumerate(dataset.classes):\n",
    "        tokens = tokenizer(name, \n",
    "                           return_tensors=\"pt\",\n",
    "                           padding=True, \n",
    "                           truncation=True, \n",
    "                           max_length=max_length).to(\"cuda\")\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            text_embedding = text_encoder(**tokens).last_hidden_state[:, 0, :].squeeze(0).cpu()  # Move back to CPU\n",
    "            text_embeddings[idx] = {\n",
    "                \"name\" : name,\n",
    "                \"emb\" : text_embedding,\n",
    "                \"idx\" : idx, \n",
    "            }\n",
    "    \n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(text_embeddings, f)\n",
    "\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf6aea-6aca-4495-8f35-19a37e22285f",
   "metadata": {},
   "source": [
    "## flickr30k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7eafc9-a258-4514-a60d-1eb3fd9358a6",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10fd1b8-cae0-4146-b925-852741f27013",
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr30k_target_dir = \"data/flickr30k\"\n",
    "os.makedirs(f\"{flickr30k_target_dir}/embs\", exist_ok=True)\n",
    "flickr30k_target_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cb8863-9d97-4f8e-baf9-1294e8f05412",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings_flickr30k = image_embedder(f\"{flickr30k_target_dir}/flickr30k_images\", f\"{flickr30k_target_dir}/embs/flickr30k_images.pkl\")\n",
    "len(list(image_embeddings_flickr30k.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4159e56-095d-41b1-b5f3-67ffc4aba132",
   "metadata": {},
   "source": [
    "### Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b1459-771c-4c7c-a516-6c9d97848d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr30k_text_embeddings = flickr30k_text_embedder(f\"{flickr30k_target_dir}/captions.txt\", f\"{flickr30k_target_dir}/embs/flickr30k_captions.pkl\")\n",
    "len(list(flickr30k_text_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e68d6-e2d7-43df-b224-1c99c203173d",
   "metadata": {},
   "source": [
    "## CIFAR-10, CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5729e38-1ac1-4dac-9e9d-c9aed0f10989",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_target_dir = \"data/cifar\"\n",
    "os.makedirs(f\"{cifar_target_dir}/embs\", exist_ok=True)\n",
    "cifar_target_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9175201-9c0d-4be4-a913-c9e48087e0c3",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef635e-d671-4e29-8c4b-d4a5d14840d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(root=cifar_target_dir, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc22d4-c2f1-4c58-beb3-5c63cc89e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings_cifar10 = cifar_image_embedder(cifar10, f\"{cifar_target_dir}/embs/cifar10_images.pkl\")\n",
    "len(list(image_embeddings_cifar10.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897780fa-bcad-4d17-8ad0-52084e47ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100 = datasets.CIFAR100(root=cifar_target_dir, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd31735-59e5-4019-8821-e0311c781558",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings_cifar100 = cifar_image_embedder(cifar100, f\"{cifar_target_dir}/embs/cifar100_images.pkl\")\n",
    "len(list(image_embeddings_cifar100.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9fdfc-326d-46ba-9e3e-926e51f0cb68",
   "metadata": {},
   "source": [
    "### Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ecbd9-add1-4185-8420-c99731d786df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_text_embeddings_train = cifar_text_embedder(cifar10, f\"{cifar_target_dir}/embs/cifar10_captions.pkl\")\n",
    "len(list(cifar10_text_embeddings_train.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de35f08-4722-48a9-a3df-971599201f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_text_embeddings_train = cifar_text_embedder(cifar100, f\"{cifar_target_dir}/embs/cifar100_captions.pkl\")\n",
    "len(list(cifar100_text_embeddings_train.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ca352-0743-45bd-9d56-9c2396ca6f2a",
   "metadata": {},
   "source": [
    "## Coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2278438-4599-4389-9015-f9b4096132db",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_target_dir = \"data/coco\"\n",
    "os.makedirs(f\"{coco_target_dir}/embs\", exist_ok=True)\n",
    "coco_target_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f7679-4dd1-4d87-aa21-c3b365dc5f70",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0abfb-c27f-4c9f-8ff4-e73a052f20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_image_embeddings_val = image_embedder(f\"{coco_target_dir}/val2017/val2017\", f\"{coco_target_dir}/embs/val2017_images.pkl\")\n",
    "len(list(coco_image_embeddings_val.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d898cbd4-087b-4ed8-a4ea-5d8b602c7da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_image_embeddings_train = image_embedder(f\"{coco_target_dir}/train2017/train2017\", f\"{coco_target_dir}/embs/train2017_images.pkl\")\n",
    "len(list(coco_image_embeddings_train.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86bfc35-5fd6-4763-ad6c-7240e2131068",
   "metadata": {},
   "source": [
    "### Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391bdbc7-a3ba-4beb-b8c0-d2398f4bcbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_text_embeddings_val = coco_text_embedder(f\"{coco_target_dir}/annotations/annotations/captions_val2017.json\", f\"{coco_target_dir}/embs/val2017_captions.pkl\")\n",
    "len(list(coco_text_embeddings_val.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9364e5-9540-4b67-baee-c45561787b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_text_embeddings_train = coco_text_embedder(f\"{coco_target_dir}/annotations/annotations/captions_train2017.json\", f\"{coco_target_dir}/embs/train2017_captions.pkl\")\n",
    "len(list(coco_text_embeddings_train.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16feda26-e27a-4d64-a544-05bfae2d304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clip]",
   "language": "python",
   "name": "conda-env-clip-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
