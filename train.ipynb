{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204da7d7-e36f-4a59-bff9-5e62ed0dfbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10947f30-6423-4e2f-bba7-6a87e46b49ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class PrecomputedDataset(Dataset):\n",
    "    def __init__(self, emb_images_file, emb_captions_file, device=\"cuda\"):\n",
    "        with open(emb_images_file, \"rb\") as f:\n",
    "            self.eimg = pickle.load(f)\n",
    "\n",
    "        with open(emb_captions_file, \"rb\") as f:\n",
    "            self.etext = pickle.load(f)\n",
    "\n",
    "        self.device = device\n",
    "        self.keys = list(self.etext.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        key_text = self.keys[idx]\n",
    "        # print(key_text)\n",
    "        # Feltételezzük, hogy self.etext[key_text][\"emb\"] tensor formátumú\n",
    "        tensor_text = self.etext[key_text][\"emb\"].clone().detach().to(self.device)\n",
    "        \n",
    "        key_img = self.etext[key_text][\"img_fn\"]\n",
    "        # print(key_img)\n",
    "        # Feltételezzük, hogy self.eimg[key_img] tensor formátumú\n",
    "        tensor_img = self.eimg[key_img].clone().detach().to(self.device)       \n",
    "        \n",
    "        return tensor_img, tensor_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0fdff3-1d46-45b4-a440-6b93d262ff56",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, image_encoder_dim, text_encoder_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_proj = nn.Linear(image_encoder_dim, output_dim)\n",
    "        self.text_proj = nn.Linear(text_encoder_dim, output_dim)\n",
    "        self.embed_dim = output_dim\n",
    "\n",
    "        print(image_encoder_dim, text_encoder_dim, output_dim)\n",
    "\n",
    "    def forward(self, image_tensor, text_tensor):\n",
    "        # Projection\n",
    "        image_embeddings = self.image_proj(image_tensor)\n",
    "        text_embeddings = self.text_proj(text_tensor)\n",
    "    \n",
    "        # Normalization\n",
    "        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "        text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_embeddings, text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f48df116-f18a-4c6c-a781-ec9ee7922039",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CLIPLoss(nn.Module):\n",
    "    def __init__(self, initial_temperature=0.07):\n",
    "        super(CLIPLoss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        # Define the temperature parameter as a learnable parameter\n",
    "        self.temperature = nn.Parameter(torch.tensor(initial_temperature))\n",
    "\n",
    "    def forward(self, image_features, text_features):\n",
    "        # Scale logits with learnable temperature\n",
    "        logits = torch.matmul(image_features, text_features.t()) / self.temperature\n",
    "        labels = torch.arange(logits.shape[0], device=logits.device)\n",
    "        loss = (self.loss_fn(logits, labels) + self.loss_fn(logits.t(), labels)) / 2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42e17ec-d5e8-476a-b65e-ee97403adaee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CLIPLossWithCompensation(nn.Module):\n",
    "    def __init__(self, initial_temperature=0.07, initial_alpha=0.5):\n",
    "        super(CLIPLossWithCompensation, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        # Define the temperature parameter as a learnable parameter\n",
    "        self.temperature = nn.Parameter(torch.tensor(initial_temperature))\n",
    "        # Scaling factor for the compensation\n",
    "        self.alpha = nn.Parameter(torch.tensor(initial_alpha)) # alpha\n",
    "\n",
    "    def forward(self, image_features, text_features):\n",
    "        # Compute logits scaled with learnable temperature\n",
    "        logits = torch.matmul(image_features, text_features.t()) / self.temperature\n",
    "        \n",
    "        # Normalize the features to compute cosine similarity\n",
    "        image_features_norm = F.normalize(image_features, p=2, dim=-1)\n",
    "        text_features_norm = F.normalize(text_features, p=2, dim=-1)\n",
    "\n",
    "        # Compute pairwise cosine similarity for images and texts\n",
    "        image_sim = torch.matmul(image_features_norm, image_features_norm.t())  # Image-to-image similarity\n",
    "        text_sim = torch.matmul(text_features_norm, text_features_norm.t())    # Text-to-text similarity\n",
    "\n",
    "        # Compute dynamic weights for the logits based on similarity\n",
    "        weights = 1 + self.alpha * (image_sim + text_sim) / 2  \n",
    "\n",
    "        # Apply weights to the logits\n",
    "        weighted_logits = logits * weights\n",
    "\n",
    "        # Create labels for the positive pairs\n",
    "        labels = torch.arange(logits.shape[0], device=logits.device)\n",
    "\n",
    "        # Compute the loss for both directions\n",
    "        loss = (self.loss_fn(weighted_logits, labels) + self.loss_fn(weighted_logits.t(), labels)) / 2\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed825d-1057-474b-9743-4e4a9fc829cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3845598-e1ca-4e17-9583-2f49dd780d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 5000, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val = PrecomputedDataset(emb_images_file=\"data/coco/embs/val2017_images.pkl\",\n",
    "                                 emb_captions_file=\"data/coco/embs/val2017_captions.pkl\")\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=32, shuffle=True)\n",
    "len(dataloader_val), len(list(dataset_val.eimg.keys())), len(list(dataset_val.etext.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b558bb9-99f9-45b4-b987-6fffad90c947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3697, 118287, 118287)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = PrecomputedDataset(emb_images_file=\"data/coco/embs/train2017_images.pkl\",\n",
    "                                   emb_captions_file=\"data/coco/embs/train2017_captions.pkl\")\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "len(dataloader_train), len(list(dataset_train.eimg.keys())), len(list(dataset_train.etext.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ca4eb9-f105-4639-8282-3f78b62db5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40c85091-41bf-4b75-8bb8-3027c8d0a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loss_fn, epoch, mark):\n",
    "    model = CLIPModel(image_encoder_dim=2048, text_encoder_dim=768, output_dim=512)\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "    optimizer = optim.Adam(list(model.parameters()) + list(loss_fn.parameters()), lr=1e-4, weight_decay=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCH)\n",
    "\n",
    "    current_time = datetime.datetime.now()\n",
    "    directory_name = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    directory_name_models = f\"models/{mark}_{directory_name}\"\n",
    "    directory_name_results = f\"results/{mark}_{directory_name}\"\n",
    "    os.mkdir(directory_name_models, exist_ok=True)\n",
    "    os.mkdir(directory_name_results, exist_ok=True)\n",
    "    print(directory_name_models, directory_name_results)\n",
    "    \n",
    "    pbar = tqdm(range(epoch))\n",
    "    history = []\n",
    "    val_loss = 0.0\n",
    "    val_loss_best = float(\"inf\")\n",
    "    val_accuracy = 0.0\n",
    "    val_accuracy_best = 0.0\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    for ep in pbar:\n",
    "        # Train\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for batch, (tesnor_img, tensor_text) in enumerate(dataloader_train):\n",
    "            optimizer.zero_grad()\n",
    "            image_features, text_features = model(tesnor_img, tensor_text)\n",
    "            loss = loss_fn(image_features, text_features)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = loss.item()\n",
    "\n",
    "            h = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"temperature\": loss_fn.temperature.item(),\n",
    "                \"alpha\": 0.0,\n",
    "                \"batch\": batch + 1,\n",
    "                \"learning rate\": current_lr,\n",
    "                \"epoch\": ep\n",
    "            }\n",
    "    \n",
    "            if hasattr(loss_fn, \"alpha\"):\n",
    "                h[\"alpha\"] = loss_fn.alpha.item()\n",
    "    \n",
    "            history.append(h)\n",
    "            train_loss = round(train_loss, 5)\n",
    "            pbar.set_description(f\"Train Loss: {train_loss}, Val Loss: {val_loss}, Best Acc: {val_accuracy_best}, Best V. Loss: {val_loss_best}\")\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "        val_accuracy = 0.0\n",
    "        all_cosine_sim = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for tensor_img, tensor_text in dataloader_val:\n",
    "                image_features, text_features = model(tensor_img, tensor_text)\n",
    "                loss = loss_fn(image_features, text_features)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "                # Cosine similarity for validation metrics\n",
    "                cosine_sim = torch.matmul(image_features, text_features.T).cpu().numpy()\n",
    "                all_cosine_sim.append(cosine_sim)\n",
    "        \n",
    "                preds = cosine_sim.argmax(axis=1)  # Predicted indices\n",
    "                targets = np.arange(len(tensor_img))  # Ground truth: perfect match\n",
    "                # print(\"p\",preds)\n",
    "                # print(\"t\",targets)\n",
    "                val_accuracy += accuracy_score(targets, preds)\n",
    "        \n",
    "            # Average validation metrics\n",
    "            val_loss /= len(dataloader_val)\n",
    "            val_accuracy /= len(dataloader_val)\n",
    "        \n",
    "            # Combine similarity matrices by padding to maximum shape\n",
    "            max_rows = max(sim.shape[0] for sim in all_cosine_sim)\n",
    "            max_cols = max(sim.shape[1] for sim in all_cosine_sim)\n",
    "            padded_cosine_sim = np.zeros((max_rows, max_cols))\n",
    "        \n",
    "            for sim in all_cosine_sim:\n",
    "                padded_cosine_sim[:sim.shape[0], :sim.shape[1]] += sim\n",
    "        \n",
    "            cosine_sim_matrix = padded_cosine_sim / len(all_cosine_sim)\n",
    "        \n",
    "            # Visualization\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cosine_sim_matrix, cmap=\"viridis\", annot=False)\n",
    "            plt.title(f\"Epoch:{ep} Val Acc: {val_accuracy:.4f}, Loss: {val_loss:.4f}\")\n",
    "            plt.xlabel(\"Text Embeddings\")\n",
    "            plt.ylabel(\"Image Embeddings\")\n",
    "            plt.savefig(f\"{directory_name_results}/similarity_matrix_epoch_{ep:02}.png\")\n",
    "            plt.close()\n",
    "\n",
    "            h = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"temperature\": loss_fn.temperature.item(),\n",
    "                \"alpha\": 0.0,\n",
    "                \"batch\": 0,\n",
    "                \"learning rate\": current_lr,\n",
    "                \"epoch\": ep,\n",
    "            }\n",
    "\n",
    "            if hasattr(loss_fn, \"alpha\"):\n",
    "                h[\"alpha\"] = loss_fn.alpha.item()\n",
    "\n",
    "            history.append(h)\n",
    "            pd.DataFrame(history).to_csv(f\"{directory_name_results}/history.csv\", index=False)\n",
    "\n",
    "            # Refresh Learning rate\n",
    "            scheduler.step()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "            # Save model\n",
    "            train_loss = round(train_loss, 5)\n",
    "            val_loss = round(val_loss, 5)\n",
    "            if val_loss_best >= val_loss:\n",
    "                val_loss_best = val_loss\n",
    "                val_accuracy_best = val_accuracy\n",
    "        \n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, f\"{directory_name_models}/model.pth\")\n",
    "        \n",
    "            pbar.set_description(f\"Train Loss: {train_loss}, Val Loss: {val_loss}, Best Acc: {val_accuracy_best}, Best V. Loss: {val_loss_best}\")\n",
    "\n",
    "    del model\n",
    "    del optimizer\n",
    "    del scheduler\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b194e9-0e08-42d8-acc6-33122a262ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048 768 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/clip/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory name: models/compensated_20241217092922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0477, Val Loss: 0.0, Best Acc: 0.0, Best V. Loss: inf:   0% 0/50 [00:09<?, ?it/s] "
     ]
    }
   ],
   "source": [
    "train(loss_fn=CLIPLossWithCompensation(), epoch=EPOCH, mark=\"compensated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d620e7c2-702a-490f-942f-5b43e0357c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048 768 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/clip/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory name: models/baseline_20241217101413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.00495, Val Loss: 1.23966, Best Acc: 0.6711783439490446, Best V. Loss: 0.96155: 100% 50/50 [13:40<00:00, 16.41s/it]\n"
     ]
    }
   ],
   "source": [
    "train(loss_fn=CLIPLoss(), epoch=EPOCH, mark=\"baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0950e3c2-c561-452b-b35f-e0e72249c5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b1cce-c627-403a-8b1a-cf416c6d15b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clip]",
   "language": "python",
   "name": "conda-env-clip-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
